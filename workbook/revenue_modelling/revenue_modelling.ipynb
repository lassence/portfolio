{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Revenue modelling for client X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context\n",
    "\n",
    "For Google Ads (formerly named Adwords), one way to try to enhance performance is to activate [automated bidding](https://support.google.com/google-ads/answer/2979071). Indeed, as SEA campaigns have increased in granularity over the years, they have equivalently grown in complexity, to the point where manual campaign management becomes too difficult. Automated bidding strategies are powered by Machine Learning algorithms and can handle such complexity better than humans, with the promise of better performance.\n",
    "\n",
    "Several automated strategies are available in Google Ads, from maximising *Impression Share* to optimising *Return On Ad Spend* (ROAS). For client X, as the ultimate goal of SEA campaigns is to generate subscriptions to their service, we are interested in the subset of Google Ads bidding strategies that are focused on *conversions*. \n",
    "\n",
    "However, on client X websites, user can only go as far as pre-subscription, also called *leads*. The actual outcome (granted/rejected + revenue) will not be tracked on the website, and will not be available in Google Ads for campaigns optimisation. Even though it is technically possible to import this information into Google Ads, this is not an option for client X due to legal and privacy constraints. \n",
    "\n",
    "Thus, our goal is to maximise the actual *revenue* generated by Google Ads, while reporting performance only as far as *leads* in the platform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objective of the analysis\n",
    "\n",
    "For the Google Ads client X campaigns in France, the current conversion funnel can be schematised as:   \n",
    "`Impressions -> Clicks -> Leads -> Conversion & Revenue`\n",
    "\n",
    "The tracking in Google Ads goes only up to *Lead* and does not include any data about actual *Conversions* and associated *Revenue*.\n",
    "\n",
    "Our goal is to provide Google Ads with an estimation of the expected revenue for every Lead, so that bidding can be optimised on revenue (*Target ROAS* strategy) and not only on Leads (*Target CPA* strategy). This estimated value can then be injected in the Google Ads [tracking tag](https://support.google.com/google-ads/answer/6095947) implemented on the website, thus reporting a dummy revenue value to the Google Ads algorithms for each Lead. \n",
    "\n",
    "In this analysis, we try to identify the most significant dimension(s) (Product, Device, Date, etc.) to predict an average Revenue per Lead.\n",
    "\n",
    "While our estimations will by definition never be perfectly accurate, the business decision criterion should be **whether the performance gains of switching to a ROAS bidding strategy based on approximate revenue will outweigh Manual or Target CPA strategies based only on Leads**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-requisites\n",
    "\n",
    "1. A dataset that includes Google Ads performance reporting joined with the associated Conversions and Revenue (from internal sources) for the **last 12 months**, in order to have enough volume and relatively fresh data. The dimensions and metrics of the dataset are detailed in a later section.   \n",
    "\n",
    "2. Only include **non-brand campaigns** data. Brand campaigns usually appear later in the conversion funnel, and have very different performance, that should be considered separately from generic campaigns.  \n",
    "\n",
    "3. Each campaign with a Target ROAS strategy should generate at least **15 Leads over a 30 days period** (50 Leads recommended), in order for the algorithm to have enough data.   \n",
    "\n",
    "4. We have focused on **Google** Search campaigns, which usually represent most of the Search spends. However the same principle could theoretically be applied to other search engines, once this strategy has been successfully applied to Google campaigns.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Documentation and resources\n",
    "\n",
    "The [Google Ads Help website](https://support.google.com/google-ads/) provides detailed and up-to-date information, and is a privileged source of information.\n",
    "\n",
    "Specifically, the links below provide useful information for this analysis:\n",
    "- [About automated bidding](https://support.google.com/google-ads/answer/2979071)  \n",
    "- [Smart Bidding contextual signals](https://support.google.com/google-ads/answer/7065882)\n",
    "- [About Target CPA bidding](https://support.google.com/google-ads/answer/6268632)\n",
    "- [About Target ROAS bidding](https://support.google.com/google-ads/answer/6268637)\n",
    "- [About transactions value](https://support.google.com/google-ads/answer/3419241)\n",
    "- [Track transaction-specific value](https://support.google.com/google-ads/answer/6095947)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Standard libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "### Machine Learning libraries\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import classification_report\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
    "\n",
    "### Graph setup\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "sns.set();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Load data\n",
    "df = pd.read_csv('../data/raw_dataset.csv', low_memory=False)\n",
    "\n",
    "### Look at columns and number of rows\n",
    "df.info(verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df = (\n",
    "    df\n",
    "    # Filter out rows with zero impressions, except if they have a cost or a Lead\n",
    "    .loc[lambda x: (x['Impressions'] > 0) | (x['Cost'] > 0) | (x['Leads'] > 0)]\n",
    "    .assign(Cost=lambda x: x['Cost']/1000000,\n",
    "            Date=lambda x: pd.to_datetime(x['Day']),\n",
    "            CampaignNetwork=lambda x: x['Campaign'].str.extract(r'^([A-Z]+)_'),\n",
    "            CampaignBrand=lambda x: x['Campaign'].str.contains(r'_Marque_'),\n",
    "            CampaignMobile=lambda x: x['Campaign'].str.contains(r'_Mobile'),\n",
    "            Campaign=lambda x: x['Campaign'].apply(lambda y: str(hash(y))),   # Anonymize campaign names\n",
    "            KwUniqueID=(df['Ad group ID'].astype(str) + df['Keyword ID'].astype(str)),\n",
    "            Week=lambda x: x['Date'].dt.isocalendar().week,\n",
    "            WeekDay=lambda x: x['Date'].dt.weekday,\n",
    "            MonthDay=lambda x: x['Date'].dt.day)\n",
    "    .loc[:, ['CampaignNetwork', 'CampaignBrand', 'CampaignMobile', 'Campaign', 'Adgroup', \n",
    "             'Keyword', 'KwUniqueID', 'Week', 'MonthDay', 'WeekDay', 'Date', 'Cost', \n",
    "             'Impressions', 'Clicks', 'Leads', 'Convs', 'Revenue', 'Product']]\n",
    "    # Zero Leads generated by brand campaigns and GDN campaigns ==> filter them out\n",
    "    .loc[lambda x: (x['CampaignBrand'] == False) & (x['CampaignNetwork'] == 'SEA')]\n",
    "    .sort_values(by=['Date', 'Campaign', 'Adgroup', 'Keyword'])\n",
    "    .reset_index(drop=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_df.info(verbose=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This transformed dataset leaves us with a smaller table, with 185K rows and 18 columns, that we can now use for further analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics evolution by day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Aggregate at day level\n",
    "daily_df = (\n",
    "    clean_df\n",
    "    .loc[:, 'Date':]\n",
    "    .groupby(['Date'])\n",
    "    .sum()\n",
    "    .assign(LeadRate=lambda x: x['Leads']/x['Clicks'],  # Leads/Clicks\n",
    "            ConvRate=lambda x: x['Convs']/x['Leads'],   # Convs/Leads\n",
    "            RevLead=lambda x: x['Revenue']/x['Leads'])  # Revenue/Leads\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Plot key metrics by day\n",
    "daily_df.plot(y=['Cost', 'Leads', 'Convs', 'Revenue'], subplots=True, sharex=False, figsize=(14,14));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Check absence of correlation between Leads/Clicks and Convs/Convs\n",
    "sns.pairplot(daily_df[['ConvRate', 'LeadRate']])\n",
    "print(\"Correlation coefficient between LeadRate and ConvRate: {:.4f}\".format(daily_df['ConvRate'].corr(daily_df['LeadRate'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Look at summary metrics\n",
    "daily_df.agg(['sum', 'mean', 'median', 'std', 'min', 'max'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see above, while Adwords-related data is available from DD/MM/YYYY up to DD/MM/YYYY, we only have **conversion data up to MM YYYY**, e.g. about x months of data.  \n",
    "\n",
    "Specifically on conversion data, we have **x Leads, y Convs and z€ revenue**. The overall \"Conversion Rate\" (`ConvRate` = `Convs`/`Leads`) is x%, and the average Revenue per Lead (`RevLead` = `Revenue`/`Leads`) is x€."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Remove all data after DD/MM, as there is no conversion\n",
    "clean_df = clean_df.loc[lambda x: x['Date'] <= '2018-08-01', :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distribution of production metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be interested in the dispersion of `ConvRate` and `RevLead` metrics, so let's have a look at their overall distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Look at distribution of final conversions\n",
    "fig, ax = plt.subplots(2, 1, figsize=(12, 8))\n",
    "\n",
    "sns.histplot(daily_df['ConvRate'].dropna(), ax=ax[0])\n",
    "ax[0].axvline(daily_df['ConvRate'].mean(), color='r')\n",
    "\n",
    "sns.histplot(daily_df['RevLead'].dropna(), color='g', ax=ax[1])\n",
    "ax[1].axvline(daily_df['RevLead'].mean(), color='r');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conversions by campaigns, adgroups and keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One important aspect for conversions estimations is how granular we can go down the Adwords structure, which is correlated to how many conversions we have at Campaign/Adgroup/Keyword level. Let's have a look at conversions volumes by level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adwords_structure_df = (\n",
    "    clean_df\n",
    "    .groupby(['Campaign', 'Adgroup', 'Keyword', 'KwUniqueID'])\n",
    "    .agg(sum)\n",
    "    .loc[:, 'Cost':'Revenue']\n",
    "    .sort_values('Convs', ascending=False)\n",
    "    .reset_index()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(14, 6))\n",
    "n = 30\n",
    "pd.concat([\n",
    "(\n",
    "    adwords_structure_df\n",
    "    .groupby('Campaign').sum()\n",
    "    .sort_values('Convs', ascending=False)\n",
    "    .reset_index()\n",
    "    .loc[:n, ['Convs']]\n",
    "    .rename(columns={'Convs': 'Top Campaign Convs'})\n",
    "),\n",
    "(\n",
    "    adwords_structure_df\n",
    "    .groupby(['Campaign', 'Adgroup']).sum()\n",
    "    .sort_values('Convs', ascending=False)\n",
    "    .reset_index()\n",
    "    .loc[:n, ['Convs']]\n",
    "    .rename(columns={'Convs': 'Top Adgroup Convs'})\n",
    "),\n",
    "(\n",
    "    adwords_structure_df\n",
    "    .groupby(['Campaign', 'Adgroup', 'Keyword']).sum()\n",
    "    .sort_values('Convs', ascending=False)\n",
    "    .reset_index()\n",
    "    .loc[:n, ['Convs']]\n",
    "    .rename(columns={'Convs': 'Top Keyword Convs'})\n",
    ")\n",
    "], axis=1).plot(kind='bar', ax=ax)\n",
    "ax.axhline(10, color='gray', linestyle='--');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we define the **minimum number of Convs at 10** to calculate meaningful, statistically significant metrics, we will be limited by our dataset to the top 16 Campaigns, after which the number of observed Convs drops below 10."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Estimate features importances with 'dummy' models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Several dimensions may intuitively be analysed, to look at how they affect average Revenue per Lead: day of month, day of week, device, campaign. We choose to not go more granular than Campaign (i.e. not at Adgroup or Keyword level), as there would be too many values, and consequently too small volumes, as seen in the graph above.\n",
    "\n",
    "However, to have a hint at which dimensions may be the most \"discriminant\" in explaining the Revenue per Lead, we can mimic a simple Machine Learning model for rows with exactly 1 Lead. While the model will be very simple and not exploitable for actual prediction, it may guide us to the prominent dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Keep only rows with Leads == 1, to mimic non-aggregated data\n",
    "model_df = (\n",
    "    clean_df\n",
    "    .loc[lambda x: x['Leads'] == 1, :]\n",
    "    .assign(Converted=lambda x: (x['Convs'] > 0).astype('int'),\n",
    "            CampaignDummy=clean_df['Campaign'].astype('category').cat.codes,\n",
    "            CampaignMobile=clean_df['CampaignMobile'].astype('int'))\n",
    "    .loc[:, ['Campaign', 'CampaignDummy', 'CampaignMobile', 'WeekDay', 'MonthDay', 'Converted', 'Revenue']]\n",
    "    .reset_index(drop=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Split into features and target datasets\n",
    "np.random.seed(2222)\n",
    "X = model_df.iloc[:, 1:-2]\n",
    "y = model_df.iloc[:, -1]\n",
    "\n",
    "### Split into training and test datasets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Utility function for fitting model and displaying features importance\n",
    "def show_features(model):\n",
    "    \n",
    "    # Fit model\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # Predict on test set\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Features importance\n",
    "    feature_importance = model.feature_importances_\n",
    "    sorted_idx = np.argsort(feature_importance)\n",
    "\n",
    "    # Plot features\n",
    "    pos = np.arange(sorted_idx.shape[0]) + .5\n",
    "    plt.barh(pos, feature_importance[sorted_idx], align='center')\n",
    "    plt.yticks(pos, X.columns[sorted_idx])\n",
    "    title = 'R²: ' + str(round(model.score(X_test, y_test), 4))\n",
    "    plt.title(title);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2222)\n",
    "show_features(GradientBoostingRegressor())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(2222)\n",
    "show_features(RandomForestRegressor(n_estimators=100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insights from the features importance estimation\n",
    "\n",
    "Both the Gradient Boosting and Random Forest classify `MonthDay` and `CampaignDummy` as the two most significant dimensions in estimating if a Lead will lead to a Conversion. Also, `CampaignMobile` seems to be a poorly discriminant feature.\n",
    "\n",
    "The following remarks can be made:  \n",
    "1. Based on the current Adwords structure, **`Campaigns` include several pieces of information**:  \n",
    "    1. *Device*: according to the current design, Mobile and Desktop/Tablets campaigns are separated. Thus, `Campaign` and `CampaignMobile` are strongly intercorrelated.  \n",
    "    2. *Product*: thematic-related campaigns, e.g. `XXXX`, probably induce a higher probability of converting on a specific product.  \n",
    "    3. *Quality of the `Leads`*: it might be that very specific queries (`xxxx`) come from more engaged users than from more generic keywords (e.g. `xxxx`)  \n",
    "\n",
    "2. `MonthDay` seem to be a relevant feature, but it includes **31 different values, which may be too granular** combined with one or more other dimensions, and lead to small `Convs` volumes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conversion KPIs broken down by dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now dive deeper in the 4 dimensions (`Campaign`, `MonthDay`, `WeekDay`, `CampaignMobile`) and look at their distribution. We are especially interested in dimensions that have values with low dispersion.\n",
    "\n",
    "Dispersion can be calculated with the following criteria:\n",
    "- Standard Deviation\n",
    "- Coefficient of Variation = Standard Deviation / Mean\n",
    "- Interquartile Range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Helper function for plotting production KPIs by different dimensions\n",
    "def graph_dist(dimension, graph_data=None, style='violin'):\n",
    "    \n",
    "    if graph_data is None:\n",
    "        graph_data = (\n",
    "            clean_df\n",
    "            .groupby([dimension, 'Date'], as_index=False)\n",
    "            .agg(sum)\n",
    "            .assign(ConvRate=lambda x: x['Convs']/x['Leads'])\n",
    "            .assign(RevLead=lambda x: x['Revenue']/x['Leads'])\n",
    "        )\n",
    "        \n",
    "    if style == 'violin':\n",
    "        g = sns.violinplot(x=dimension, y='ConvRate', data=graph_data, cut=0, color='b', ax=ax[0])\n",
    "        g = sns.violinplot(x=dimension, y='RevLead', data=graph_data, cut=0, color='g', ax=ax[1])\n",
    "    else:\n",
    "        g = sns.boxplot(x=dimension, y='ConvRate', data=graph_data, width=0.3, showfliers=False, color='b', ax=ax[0])\n",
    "        g = sns.boxplot(x=dimension, y='RevLead', data=graph_data, width=0.3, showfliers=False, color='g', ax=ax[1])        \n",
    "    \n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Function to extract main dispersion metrics for a given dimension\n",
    "def summary_dim(dimension):\n",
    "    \n",
    "    df = (\n",
    "        clean_df\n",
    "        .assign(ConvRate=lambda x: x['Convs']/x['Leads'])\n",
    "        .assign(RevLead=lambda x: x['Revenue']/x['Leads'])\n",
    "        .loc[:, [dimension, 'Leads', 'Convs', 'Revenue', 'ConvRate', 'RevLead']]\n",
    "        .groupby(dimension, as_index=True)\n",
    "        .agg({'Leads': 'sum', 'Convs': 'sum', 'Revenue': 'sum', \n",
    "              'ConvRate': ['mean', 'std'], \n",
    "              'RevLead' : ['mean', 'std']\n",
    "             })\n",
    "    )\n",
    "    df['ConvRate', 'coeff_var'] = df['ConvRate', 'std']/df['ConvRate', 'mean']\n",
    "    df['RevLead', 'coeff_var'] = df['RevLead', 'std']/df['RevLead', 'mean']\n",
    "    for col in [0, 1, 2]:\n",
    "        df.iloc[:, col] = df.iloc[:, col].astype(int)\n",
    "    for col in [3, 4]:\n",
    "        df.iloc[:, col] = df.iloc[:, col].round(4)\n",
    "    for col in [5, 6, 7, 8]:\n",
    "        df.iloc[:, col] = df.iloc[:, col].round(2)\n",
    "        \n",
    "    return df.iloc[:, [0,1,2,3,4,7,5,6,8]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### By device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(14, 6))\n",
    "graph_dist('CampaignMobile');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_dim('CampaignMobile')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### By day of week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(14, 6))\n",
    "graph_dist('WeekDay');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "summary_dim('WeekDay')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### By day of month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 1, figsize=(14, 12))\n",
    "graph_dist('MonthDay', style='box');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_dim('MonthDay').head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### By top campaign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Group by week, to have enough data by campaign\n",
    "top_campaigns_df = (\n",
    "    clean_df\n",
    "    .groupby('Campaign', as_index=False)\n",
    "    .sum()\n",
    "    .sort_values('Convs', ascending=False)\n",
    "    .head(15)\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    "graph_df = (\n",
    "    clean_df\n",
    "    .loc[clean_df['Campaign'].isin(top_campaigns_df['Campaign'])]\n",
    "    .groupby(['Campaign', 'Week'], as_index=False)\n",
    "    .agg(sum)\n",
    "    .assign(ConvRate=lambda x: x['Convs']/x['Leads'])\n",
    "    .assign(RevLead=lambda x: x['Revenue']/x['Leads'])\n",
    ")\n",
    "fig, ax = plt.subplots(2, 1, figsize=(14, 12))\n",
    "graph_dist('Campaign', graph_data=graph_df, style='box')\n",
    "[label.set_rotation(90) for axe in ax for label in axe.get_xticklabels()]\n",
    "plt.setp(ax[0].get_xticklabels(), visible=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crossing dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_dim(dimensions):\n",
    "\n",
    "    graph_df = (\n",
    "        clean_df\n",
    "        .groupby(dimensions)\n",
    "        .agg({'Leads': ['sum', 'mean', 'std']})\n",
    "    )\n",
    "    graph_df.columns = graph_df.columns.get_level_values(1)\n",
    "    graph_df['coeff_var'] = graph_df['std']/graph_df['mean']\n",
    "\n",
    "    g = sns.heatmap(graph_df['sum'].unstack(), annot=True, fmt='.0f', cmap='rocket', ax=ax[0])\n",
    "    g = sns.heatmap(graph_df['coeff_var'].unstack(), annot=True, fmt='.3f', cmap=\"Greens_r\", ax=ax[1])\n",
    "    return g"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weekday x Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(14, 6))\n",
    "cross_dim(['WeekDay', 'CampaignMobile']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monthday x Mobile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(14, 6))\n",
    "cross_dim(['MonthDay', 'CampaignMobile']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monthday x Weekday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(14, 6))\n",
    "cross_dim(['MonthDay', 'WeekDay']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering by cross-dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_df = (\n",
    "    clean_df\n",
    "    .groupby(['MonthDay', 'CampaignMobile'])\n",
    "    .agg(sum)\n",
    "    .loc[:, ['Leads', 'Convs', 'Revenue']]\n",
    "    .assign(RevLead=lambda x: x['Revenue']/x['Leads'])\n",
    ")\n",
    "sns.clustermap(graph_df['RevLead'].unstack());"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering days of month by similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 1, figsize=(14, 14))\n",
    "\n",
    "### Prepare data\n",
    "graph_df = (\n",
    "    clean_df\n",
    "    .groupby('MonthDay')\n",
    "    .agg(sum)\n",
    "    .loc[:, ['Leads', 'Convs', 'Revenue']]\n",
    "    .assign(RevLead=lambda x: x['Revenue']/x['Leads'])\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "### Dendogram for clustering\n",
    "Z = linkage(graph_df[['RevLead']], 'single')\n",
    "dendrogram(Z, labels=range(1, 32), color_threshold=10,\n",
    "           leaf_rotation=0, leaf_font_size=12, ax=ax[0]);\n",
    "\n",
    "### Compute clusters\n",
    "days_clusters = (pd.DataFrame(\n",
    "    fcluster(Z, 5, criterion='maxclust'), \n",
    "    index=list(range(1,32)), \n",
    "    columns=['MonthDayCluster'])\n",
    ").merge(\n",
    "    graph_df, how='left', \n",
    "    left_index=True, right_on='MonthDay'\n",
    ")\n",
    "\n",
    "### Plot of days sorted by RevLead\n",
    "sns.barplot(data=days_clusters, x='MonthDay', y='RevLead', hue='MonthDayCluster', \n",
    "            order=days_clusters.sort_values('RevLead', ascending=False)['MonthDay'], \n",
    "            dodge=False, ax=ax[1]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_df = (\n",
    "     clean_df\n",
    "    .groupby(['MonthDay', 'Date'], as_index=False)\n",
    "    .agg(sum)\n",
    "    .assign(ConvRate=lambda x: x['Convs']/x['Leads'],\n",
    "            RevLead=lambda x: x['Revenue']/x['Leads'])\n",
    "    .merge(days_clusters[['MonthDay', 'MonthDayCluster']], how='left', on='MonthDay')\n",
    ")\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(14,6))\n",
    "sns.boxplot(x='MonthDayCluster', y='RevLead', data=graph_df, showfliers=False, ax=ax[0])\n",
    "sns.pointplot(x='MonthDayCluster', y='RevLead', color='gold', data=graph_df, ax=ax[0])\n",
    "sns.barplot(data=days_clusters.groupby('MonthDayCluster', as_index=False).sum(), \n",
    "            x='MonthDayCluster', y='Convs');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df = (\n",
    "    clean_df\n",
    "    .merge(days_clusters[['MonthDay', 'MonthDayCluster']], on='MonthDay')\n",
    "    .groupby(['MonthDayCluster', 'Campaign'])\n",
    "    .sum()\n",
    "    .loc[:, 'Leads':]\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "### Group campaigns that have less Convs than a threshold\n",
    "threshold = 7\n",
    "final_df['CampaignGroup'] = (\n",
    "    final_df\n",
    "    .apply(lambda x: x['Campaign'] if x['Convs'] >= threshold else 'Other campaigns', axis=1)\n",
    ")\n",
    "\n",
    "### Final grouping\n",
    "final_df = (\n",
    "    final_df\n",
    "    .groupby(['MonthDayCluster', 'CampaignGroup'])\n",
    "    .agg('sum')\n",
    "    .assign(RevLead=lambda x: x['Revenue']/x['Leads'],\n",
    "            ConvRate=lambda x: x['Convs']/x['Leads'])\n",
    ")\n",
    "\n",
    "### Plot number of Convs per cluster\n",
    "fig, ax = plt.subplots(1, 1, figsize=(12, 8))\n",
    "sns.heatmap(final_df['Convs'].unstack().transpose(), annot=True, fmt='.0f', cmap='rocket');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Smart bidding minimum conversions\n",
    "\n",
    "Beyond statistical significance of Revenue per Leads, we must take into account the [minimum \n",
    "volume of conversions](https://support.google.com/google-ads/answer/6268637?hl=en) that Adwords requires to activate Smart Bidding, e.g. **~15 Leads over the last 30 days.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Volatility of dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    clean_df\n",
    "    .groupby(['Week'])\n",
    "    .agg('sum')\n",
    "    .reset_index()\n",
    "    .loc[:, ['Week', 'Cost', 'Leads', 'Convs', 'Revenue']]\n",
    "    .assign(ConvRate=lambda x: x['Convs']/x['Leads'],\n",
    "            RevLead=lambda x: x['Revenue']/x['Leads'])\n",
    ").plot(x='Week', y=['Convs', 'RevLead'], subplots='True', figsize=(14,10));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_df = (\n",
    "    clean_df\n",
    "    .groupby(['Campaign', 'Week'])\n",
    "    .agg('sum')\n",
    "    .reset_index()\n",
    "    .loc[lambda x: x['Campaign'].isin(top_campaigns_df.loc[:5, 'Campaign']), \n",
    "         ['Campaign', 'Week', 'Cost', 'Leads', 'Convs', 'Revenue']]\n",
    "    .assign(ConvRate=lambda x: x['Convs']/x['Leads'],\n",
    "            RevLead=lambda x: x['Revenue']/x['Leads'])\n",
    ")\n",
    "\n",
    "fig, ax = plt.subplots(2, 1, figsize=(14, 12))\n",
    "sns.pointplot(data=graph_df, x='Week', y='RevLead', hue='Campaign', markers='', legend=False, ax=ax[0])\n",
    "sns.pointplot(data=graph_df, x='Week', y='ConvRate',  hue='Campaign', markers='', legend=False, ax=ax[1]);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
